import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import matplotlib.colors as mcolors
from matplotlib.lines import Line2D
from matplotlib.widgets import Slider, RadioButtons, Button # Added Button
import time
import random

# --- Configuration ---
N_VALIDATORS = 8
MODEL_NAMES = ['Model_A', 'Model_B', 'Model_C', 'Model_D']
BAD_ACTOR_ID = 0  # Validator 0 is the bad actor
TARGET_MODEL = 'Model_A' # Model the bad actor wants to promote
VICTIM_MODEL = 'Model_B' # Model the bad actor wants to sabotage

INITIAL_CREDIBILITY = 0.75
CREDIBILITY_LEARNING_RATE = 0.05 # How much credibility changes per step
CREDIBILITY_PENALTY_FACTOR = 10 # Multiplies the difference to penalize harder (Default changed to 10)
SCORE_NOISE_STDDEV = 0.05 # Standard deviation for honest validator noise
BAD_ACTOR_BIAS = 0.4 # How much the bad actor skews scores (adds for target, subtracts for victim)
MIN_CREDIBILITY = 0.01 # Floor for credibility
MAX_CREDIBILITY = 1.0

# --- Simulation State ---
# Function to initialize or reset validators
def initialize_validators(bad_actor_id):
    return {
        i: {
            'id': i,
            'credibility': INITIAL_CREDIBILITY,
            'is_bad_actor': (i == bad_actor_id)
        } for i in range(N_VALIDATORS)
    }

validators = initialize_validators(BAD_ACTOR_ID)

# Stores all submissions: list of dicts
# { 'test_id': int, 'submitter_id': int, 'submission_type': 'original'/'cross',
#   'original_validator_id': int (if type=='cross'), 'scores': {model_name: score}, 'step': int }
results_store = []

# Stores the overall calculated scores for each model
# { model_name: float }
def initialize_model_scores():
    return {model: 0.5 for model in MODEL_NAMES} # Start at neutral 0.5

final_model_scores = initialize_model_scores()
# Stores the running average of 'true' scores generated
average_true_scores = {model: 0.0 for model in MODEL_NAMES} # Initialize to 0
# Stores the number of turns each validator has taken as originator
validator_turn_counts = {i: 0 for i in range(N_VALIDATORS)}

current_step = 0
active_originator = None
active_cross_validators = []
last_test_id = -1
simulation_speed = 0.5 # Initial speed (seconds pause)

# --- Simulation Logic ---

def get_true_scores(test_id):
    """Generates 'true' scores for models on a given test."""
    # Make scores somewhat dependent on test_id for variability
    base_scores = {name: (0.5 + 0.4 * np.sin(test_id * 0.1 + ord(name[6]))) for name in MODEL_NAMES}
     # Ensure scores are within [0, 1] - use clip or rescale if necessary
    scores = {name: np.clip(score + np.random.normal(0, SCORE_NOISE_STDDEV / 5), 0, 1)
              for name, score in base_scores.items()}
    return scores

def get_validator_submission(validator_id, true_scores, is_original_submission):
    """Simulates a validator running the test and generating scores."""
    validator = validators[validator_id]
    reported_scores = {}

    for model_name, true_score in true_scores.items():
        noise = np.random.normal(0, SCORE_NOISE_STDDEV)
        reported_score = true_score + noise

        # Bad actor manipulation *only on original submission*
        if validator['is_bad_actor'] and is_original_submission:
            if model_name == TARGET_MODEL:
                reported_score += BAD_ACTOR_BIAS
            elif model_name == VICTIM_MODEL:
                reported_score -= BAD_ACTOR_BIAS

        reported_scores[model_name] = np.clip(reported_score, 0, 1) # Ensure scores stay in [0, 1]

    return reported_scores

def calculate_weighted_mean(cross_validation_submissions):
    """Calculates the credibility-weighted mean score for each model from cross-validations."""
    if not cross_validation_submissions:
        return {model: 0.5 for model in MODEL_NAMES} # Return neutral if no cross-validations

    weighted_scores = {model: 0.0 for model in MODEL_NAMES}
    total_weight = {model: 0.0 for model in MODEL_NAMES}

    for sub in cross_validation_submissions:
        validator_id = sub['submitter_id']
        credibility = validators[validator_id]['credibility']
        # Add a small epsilon to avoid division by zero if all cross-validators have 0 credibility
        credibility = max(credibility, 1e-6)

        for model_name, score in sub['scores'].items():
            weighted_scores[model_name] += score * credibility
            total_weight[model_name] += credibility

    mean_scores = {model: (weighted_scores[model] / total_weight[model]) if total_weight[model] > 0 else 0.5
                   for model in MODEL_NAMES}
    return mean_scores


def update_credibility(originator_id, original_scores, cross_validation_submissions):
    """Updates the originator's credibility based on agreement with weighted cross-validations."""
    if not cross_validation_submissions:
        return # Cannot update credibility without cross-validations

    weighted_mean_scores = calculate_weighted_mean(cross_validation_submissions)

    # Calculate the average absolute difference across all models
    total_diff = 0
    for model_name in MODEL_NAMES:
        diff = abs(original_scores[model_name] - weighted_mean_scores[model_name])
        total_diff += diff
    avg_diff = total_diff / len(MODEL_NAMES)

    # Update credibility: Decrease if difference is large, increase slightly if small
    # The penalty increases with the difference. Reward for agreement is smaller.
    # A large avg_diff (e.g., > 0.1 or 0.2) should lead to a decrease.
    # A small avg_diff (e.g., < 0.05) could lead to a slight increase.

    current_cred = validators[originator_id]['credibility']
    change_factor = 1.0 - (avg_diff * CREDIBILITY_PENALTY_FACTOR) # Penalize based on difference

    # If change_factor is positive (small diff), increase credibility slightly
    # If change_factor is negative (large diff), decrease credibility
    if change_factor > 0: # Agreement or small difference
         # Smaller reward for agreement
        credibility_change = CREDIBILITY_LEARNING_RATE * change_factor * 0.2 # Reduced reward
        new_cred = current_cred + credibility_change
    else: # Disagreement - larger penalty
        # change_factor is negative here, making credibility_change negative
        credibility_change = CREDIBILITY_LEARNING_RATE * change_factor # Use the full penalty
        new_cred = current_cred + credibility_change


    # Ensure credibility stays within bounds
    validators[originator_id]['credibility'] = np.clip(new_cred, MIN_CREDIBILITY, MAX_CREDIBILITY)
    # print(f"Step {current_step}: Validator {originator_id} Credibility updated to {validators[originator_id]['credibility']:.3f} (Avg Diff: {avg_diff:.3f})")


def calculate_final_model_scores():
    """Recalculates overall model scores based on ALL submissions and CURRENT credibilities."""
    global final_model_scores
    new_final_scores = {model: 0.0 for model in MODEL_NAMES}
    total_weights = {model: 0.0 for model in MODEL_NAMES}

    if not results_store:
        final_model_scores = {model: 0.5 for model in MODEL_NAMES} # Reset to neutral if no results
        return

    for submission in results_store:
        submitter_id = submission['submitter_id']
        # Use the LATEST credibility for weighting
        credibility = validators[submitter_id]['credibility']
        # Add epsilon to prevent division by zero if credibility is exactly 0
        credibility = max(credibility, 1e-6)

        for model_name, score in submission['scores'].items():
            new_final_scores[model_name] += score * credibility
            total_weights[model_name] += credibility

    for model_name in MODEL_NAMES:
        if total_weights[model_name] > 0:
            final_model_scores[model_name] = new_final_scores[model_name] / total_weights[model_name]
        else:
            final_model_scores[model_name] = 0.5 # Fallback to neutral


def simulation_step():
    """Performs one full step of the simulation."""
    global current_step, active_originator, active_cross_validators, last_test_id

    # 1. Select Originator (Round Robin)
    originator_id = current_step % N_VALIDATORS
    active_originator = originator_id
    validator_turn_counts[originator_id] += 1 # Increment turn count
    active_cross_validators = [i for i in range(N_VALIDATORS) if i != originator_id]
    last_test_id += 1
    # print(f"\n--- Step {current_step}: Validator {originator_id} originates Test {last_test_id} ---")

    # 2. Generate True Scores for the new test
    true_scores = get_true_scores(last_test_id)

    # 3. Originator runs test and submits
    original_scores = get_validator_submission(originator_id, true_scores, is_original_submission=True)
    results_store.append({
        'test_id': last_test_id,
        'submitter_id': originator_id,
        'submission_type': 'original',
        'original_validator_id': originator_id, # Self-reference for consistency
        'scores': original_scores,
        'step': current_step
    })
    # print(f"  Validator {originator_id} submits original: { {k: f'{v:.2f}' for k,v in original_scores.items()} }")
    if validators[originator_id]['is_bad_actor']:
         print(f"  (Bad actor {originator_id} submitted original biased scores)")


    # 4. Cross-Validators run the same test and submit
    cross_validation_submissions = []
    for cv_id in active_cross_validators:
        # Pass `is_original_submission=False` so bad actor doesn't bias cross-validations
        cv_scores = get_validator_submission(cv_id, true_scores, is_original_submission=False)
        submission = {
            'test_id': last_test_id,
            'submitter_id': cv_id,
            'submission_type': 'cross',
            'original_validator_id': originator_id,
            'scores': cv_scores,
            'step': current_step
        }
        results_store.append(submission)
        cross_validation_submissions.append(submission)
        # print(f"  Validator {cv_id} submits cross-validation.")


    # 5. Update Originator's Credibility
    update_credibility(originator_id, original_scores, cross_validation_submissions)

    # 6. Recalculate ALL final model scores retroactively
    calculate_final_model_scores()

    current_step += 1


# --- Visualization Setup ---
fig, ax = plt.subplots(figsize=(13, 9)) # Adjusted figsize for right panel
plt.subplots_adjust(left=0.05, bottom=0.20, right=0.75, top=0.98) # Reset top margin, keep bottom margin for sliders

# Validator positions (circle)
center_x, center_y = 0.5, 0.45 # Lowered center_y further to increase top space
radius = 0.35
validator_pos = {
    i: (center_x + radius * np.cos(2 * np.pi * i / N_VALIDATORS),
        center_y + radius * np.sin(2 * np.pi * i / N_VALIDATORS))
    for i in range(N_VALIDATORS)
}
validator_radius = 0.05

# Database position (Now centered)
db_width, db_height = 0.15, 0.1 # Made slightly smaller
db_pos = (center_x - db_width / 2, center_y - db_height / 2) # Centered
db_center = (db_pos[0] + db_width / 2, db_pos[1] + db_height / 2) # Calculate center

# Create visual elements (will be updated in the loop)
validator_circles = [patches.Circle(validator_pos[i], validator_radius, fc='gray', alpha=INITIAL_CREDIBILITY) for i in range(N_VALIDATORS)]
validator_texts = [ax.text(validator_pos[i][0], validator_pos[i][1], f"V{i}\nC: {validators[i]['credibility']:.2f}",
                           ha='center', va='center', fontsize=8, color='white') for i in range(N_VALIDATORS)]
db_rect = patches.Rectangle(db_pos, db_width, db_height, fc='lightblue', ec='black', zorder=5) # Ensure DB is drawn on top
db_text = ax.text(db_pos[0] + db_width / 2, db_pos[1] + db_height / 2, "Database", ha='center', va='center', zorder=6)
# Position model scores below the database
model_score_texts = [ax.text(center_x, db_pos[1] - 0.03 * (i + 1), f"{name}: {final_model_scores[name]:.3f}",
                            ha='center', va='center', fontsize=9) for i, name in enumerate(MODEL_NAMES)] # Changed va to center for arrow alignment
step_text = ax.text(0.02, 0.98, f"Step: {current_step}", transform=ax.transAxes) # Reset y-coordinate higher
title_text = ax.text(0.5, 0.98, "", transform=ax.transAxes, ha='center', va='top') # Reset y-coordinate higher
# Add arrow indicators for target/victim models
target_arrow = ax.text(0, 0, '→', color='green', fontsize=14, ha='right', va='center', visible=False) # Arrow points right, aligned right
victim_arrow = ax.text(0, 0, '→', color='red', fontsize=14, ha='right', va='center', visible=False) # Arrow points right, aligned right

# Add lines for communication animation (initially invisible)
originator_line = Line2D([], [], color='red', lw=1.5, linestyle='--', visible=False, zorder=4)
db_to_cv_lines = [Line2D([], [], color='blue', lw=1.5, linestyle=':', visible=False, zorder=4) for _ in range(N_VALIDATORS - 1)]
cv_to_db_lines = [Line2D([], [], color='blue', lw=1.5, linestyle='-', visible=False, zorder=4) for _ in range(N_VALIDATORS - 1)]

ax.add_line(originator_line)
for line in db_to_cv_lines: ax.add_line(line)
for line in cv_to_db_lines: ax.add_line(line)

for circle in validator_circles:
    ax.add_patch(circle)
ax.add_patch(db_rect)

ax.set_xlim(0, 1)
ax.set_ylim(0, 1)
ax.set_aspect('equal', adjustable='box')
ax.axis('off')

# --- Widgets ---

# Speed Slider (Positioned below main plot)
ax_speed_slider = plt.axes([0.1, 0.10, 0.6, 0.03], facecolor='lightgoldenrodyellow') # Moved up
slider_speed = Slider(ax_speed_slider, 'Sim Speed (s)', 0.01, 2.0, valinit=simulation_speed, valstep=0.01)

def update_speed(val):
    global simulation_speed
    # Invert slider value so right means faster (shorter pause)
    simulation_speed = 2.01 - val # 2.01 to ensure it doesn't hit 0 exactly
slider_speed.on_changed(update_speed)
# Initialize speed label correctly
update_speed(simulation_speed) # Call once to set initial value

# Penalty Factor Slider
ax_penalty_slider = plt.axes([0.1, 0.06, 0.6, 0.03], facecolor='lightcoral') # Moved up
slider_penalty = Slider(ax_penalty_slider, 'Penalty Factor', 0.0, 20.0, valinit=CREDIBILITY_PENALTY_FACTOR, valstep=0.1) # Range changed to 0-20, valinit uses new default

def update_penalty_factor(val):
    global CREDIBILITY_PENALTY_FACTOR
    CREDIBILITY_PENALTY_FACTOR = val
slider_penalty.on_changed(update_penalty_factor)
# Initialize penalty factor value
update_penalty_factor(CREDIBILITY_PENALTY_FACTOR) # Call once

# Score Noise Slider
ax_noise_slider = plt.axes([0.1, 0.02, 0.6, 0.03], facecolor='lightskyblue') # Positioned at the bottom
slider_noise = Slider(ax_noise_slider, 'Noise (%)', 0.0, 20.0, valinit=SCORE_NOISE_STDDEV * 100, valstep=0.5) # Shortened label

def update_noise_stddev(val):
    global SCORE_NOISE_STDDEV
    SCORE_NOISE_STDDEV = val / 100.0 # Convert percentage back to decimal
slider_noise.on_changed(update_noise_stddev)
# Initialize noise value
update_noise_stddev(SCORE_NOISE_STDDEV * 100) # Call once with initial percentage value

# Restart Button
ax_restart = plt.axes([0.835, 0.02, 0.08, 0.04]) # Position near bottom-right of sliders
button_restart = Button(ax_restart, 'Restart ↺', color='lightgreen', hovercolor='limegreen')

def restart_simulation(event):
    """Resets the entire simulation state to initial conditions."""
    global validators, results_store, final_model_scores, average_true_scores
    global validator_turn_counts, current_step, last_test_id, active_originator, active_cross_validators
    global BAD_ACTOR_ID # Need to reset this based on radio button state

    print("\n--- Restarting Simulation ---")

    # Get the currently selected bad actor from radio buttons
    current_bad_actor_label = radio_buttons.value_selected
    BAD_ACTOR_ID = int(current_bad_actor_label[1:]) # Extract ID

    # Reset all state variables
    validators = initialize_validators(BAD_ACTOR_ID)
    results_store = []
    final_model_scores = initialize_model_scores()
    average_true_scores = {model: 0.0 for model in MODEL_NAMES}
    validator_turn_counts = {i: 0 for i in range(N_VALIDATORS)}
    current_step = 0
    last_test_id = -1
    active_originator = None
    active_cross_validators = []

    # Update visuals immediately
    step_text.set_text(f"Step: {current_step}")
    title_text.set_text("Simulation Restarted")
    # Reset validator visuals
    for i in range(N_VALIDATORS):
        cred = validators[i]['credibility']
        alpha = max(0.1, cred)
        is_bad = validators[i]['is_bad_actor']
        ec = 'darkred' if is_bad else 'black'
        lw = 3 if is_bad else 1
        validator_circles[i].set_facecolor('gray')
        validator_circles[i].set_edgecolor(ec)
        validator_circles[i].set_linewidth(lw)
        validator_circles[i].set_alpha(alpha)
        validator_texts[i].set_text(f"V{i}\nC: {cred:.2f}")
        text_color = 'black' if alpha > 0.6 else 'white'
        validator_texts[i].set_color(text_color)
    # Reset score/error display
    arrow_x_offset = -0.20
    for i, name in enumerate(MODEL_NAMES):
        score_text_obj = model_score_texts[i]
        final_score = final_model_scores[name]
        avg_true = average_true_scores[name]
        error_percent = abs(final_score - avg_true) * 100
        score_text_obj.set_text(f"{name}: {final_score:.3f} (True: {avg_true:.3f} / Err: {error_percent:.1f}%)")
        x_pos, y_pos = score_text_obj.get_position()
        if name == TARGET_MODEL: target_arrow.set_position((x_pos + arrow_x_offset, y_pos))
        elif name == VICTIM_MODEL: victim_arrow.set_position((x_pos + arrow_x_offset, y_pos))
    target_arrow.set_visible(TARGET_MODEL in MODEL_NAMES)
    victim_arrow.set_visible(VICTIM_MODEL in MODEL_NAMES)
    # Reset turn counts display
    for i in range(N_VALIDATORS):
        radio_turn_texts[i].set_text(f"({validator_turn_counts[i]})")
    # Hide animation lines
    originator_line.set_visible(False)
    for line in db_to_cv_lines: line.set_visible(False)
    for line in cv_to_db_lines: line.set_visible(False)

    fig.canvas.draw_idle()

button_restart.on_clicked(restart_simulation)


# Bad Actor Selector (Radio Buttons on the right)
ax_radio = plt.axes([0.8, 0.1, 0.15, 0.8], facecolor='lightyellow') # Positioned on the right
radio_labels = [f'V{i}' for i in range(N_VALIDATORS)]
# Add title to the radio button axes
ax_radio.set_title('Bad Actor', fontsize=10, pad=10) # Added title
radio_buttons = RadioButtons(ax_radio, radio_labels, active=BAD_ACTOR_ID)
# Add text elements for turn counts next to radio buttons
radio_turn_texts = []
# Calculate positions relative to radio button labels (requires some trial/error or knowledge of widget layout)
# We'll place them slightly to the right of the button circles within the radio axes
text_x_pos = 0.8 # Adjust as needed
for i in range(N_VALIDATORS):
    # Y position needs to align vertically with the radio buttons
    # RadioButtons positions labels from top (1.0) down.
    label_y_pos = 1.0 - (i + 0.7) * (1.0 / (N_VALIDATORS + 1)) # Approximate vertical center of label i
    turn_text = ax_radio.text(text_x_pos, label_y_pos, f"({validator_turn_counts[i]})",
                              fontsize=9, ha='left', va='center', transform=ax_radio.transAxes)
    radio_turn_texts.append(turn_text)

def update_bad_actor(label):
    """Callback function when a radio button is clicked."""
    global BAD_ACTOR_ID, validators, results_store, final_model_scores, current_step, last_test_id
    global active_originator, active_cross_validators, validator_turn_counts # Added turn counts

    new_bad_actor_id = int(label[1:]) # Extract ID from 'V#' label
    if new_bad_actor_id == BAD_ACTOR_ID:
        return # No change

    print(f"\n--- Changing Bad Actor to {new_bad_actor_id} (Continuing Simulation) ---")
    old_bad_actor_id = BAD_ACTOR_ID
    BAD_ACTOR_ID = new_bad_actor_id

    # Update the 'is_bad_actor' flag without resetting other state
    if old_bad_actor_id is not None:
        validators[old_bad_actor_id]['is_bad_actor'] = False
    validators[new_bad_actor_id]['is_bad_actor'] = True

    # Immediately update visuals for the bad actor change
    title_text.set_text(f"Bad Actor changed to V{new_bad_actor_id}") # Update title immediately
    for i in range(N_VALIDATORS):
        cred = validators[i]['credibility']
        alpha = max(0.1, cred)
        is_bad = validators[i]['is_bad_actor']
        ec = 'darkred' if is_bad else 'black'
        lw = 3 if is_bad else 1 # Line width for bad actor highlight
        validator_circles[i].set_facecolor('gray') # Reset color
        validator_circles[i].set_edgecolor(ec)
        validator_circles[i].set_linewidth(lw)
        validator_circles[i].set_alpha(alpha)
        validator_texts[i].set_text(f"V{i}\nC: {cred:.2f}")
        text_color = 'black' if alpha > 0.6 else 'white'
        validator_texts[i].set_color(text_color)

    # No need to update scores/arrows here as the simulation state isn't reset
    # Only update the visual highlight of the bad actor circles and turn counts

    # Update turn count display (this doesn't reset)
    for i in range(N_VALIDATORS):
        radio_turn_texts[i].set_text(f"({validator_turn_counts[i]})")

    fig.canvas.draw_idle() # Redraw the figure to show changes immediately

radio_buttons.on_clicked(update_bad_actor)


# --- Geometry Helpers ---
def get_circle_edge_point(center, radius, external_point):
    """Calculates the point on the circle's edge towards the external_point."""
    dx = external_point[0] - center[0]
    dy = external_point[1] - center[1]
    dist = np.sqrt(dx**2 + dy**2)
    if dist == 0: return center # Avoid division by zero
    # Calculate edge point by moving radius distance from center towards external point
    edge_x = center[0] + (dx / dist) * radius
    edge_y = center[1] + (dy / dist) * radius
    return (edge_x, edge_y)

def get_rect_edge_point(rect_center, rect_width, rect_height, external_point):
    """Calculates the intersection point on the rectangle's edge."""
    cx, cy = rect_center
    w2, h2 = rect_width / 2, rect_height / 2
    px, py = external_point

    dx = px - cx
    dy = py - cy

    if dx == 0 and dy == 0: return rect_center # Point is center

    # Check intersections with vertical edges (x = cx +/- w2)
    t_vx1 = (cx - w2 - cx) / dx if dx != 0 else float('inf')
    t_vx2 = (cx + w2 - cx) / dx if dx != 0 else float('inf')
    # Check intersections with horizontal edges (y = cy +/- h2)
    t_hy1 = (cy - h2 - cy) / dy if dy != 0 else float('inf')
    t_hy2 = (cy + h2 - cy) / dy if dy != 0 else float('inf')

    # Find the smallest positive t value (intersection point closest to rect_center)
    t_values = []
    if t_vx1 > 1e-6: # Check intersection with left edge
        iy = cy + t_vx1 * dy
        if abs(iy - cy) <= h2: t_values.append(t_vx1)
    if t_vx2 > 1e-6: # Check intersection with right edge
        iy = cy + t_vx2 * dy
        if abs(iy - cy) <= h2: t_values.append(t_vx2)
    if t_hy1 > 1e-6: # Check intersection with bottom edge
        ix = cx + t_hy1 * dx
        if abs(ix - cx) <= w2: t_values.append(t_hy1)
    if t_hy2 > 1e-6: # Check intersection with top edge
        ix = cx + t_hy2 * dx
        if abs(ix - cx) <= w2: t_values.append(t_hy2)

    if not t_values: return rect_center # Should not happen if external_point is outside

    t = min(t_values)
    return (cx + t * dx, cy + t * dy)


# --- Line Animation Helper ---
def animate_line_segments(line_objs, start_positions, end_positions, steps=5, pause_duration=0.01):
    """Animates multiple lines simultaneously from start to end positions."""
    num_lines = len(line_objs)
    if not num_lines: return
    if len(start_positions) != num_lines or len(end_positions) != num_lines:
        print("Error: Mismatched number of lines and positions in animate_line_segments")
        return

    # Generate points for all lines
    all_x_points = [np.linspace(start[0], end[0], steps) for start, end in zip(start_positions, end_positions)]
    all_y_points = [np.linspace(start[1], end[1], steps) for start, end in zip(start_positions, end_positions)]

    # Set all lines visible
    for line in line_objs:
        line.set_visible(True)

    # Animate segment by segment
    for i in range(1, steps + 1):
        for j in range(num_lines):
            line_objs[j].set_data(all_x_points[j][:i], all_y_points[j][:i])
        plt.pause(pause_duration)
    # Keep lines visible after animation completes within the phase


# --- Animation Function ---
def animate(frame):
    global active_originator, active_cross_validators # Make sure we can clear these

    # --- Run one simulation step ---
    # (Keep simulation logic separate from animation drawing)
    originator_id_this_step = current_step % N_VALIDATORS
    cross_validator_ids_this_step = [i for i in range(N_VALIDATORS) if i != originator_id_this_step]

    # Store true scores generated in this step before running the main logic
    # Note: simulation_step increments current_step *after* calculations
    current_test_id = last_test_id + 1 # Get the test ID for *this* step
    true_scores_this_step = get_true_scores(current_test_id)

    # Update running average of true scores
    # Uses current_step which reflects the count *before* this step finishes
    if current_step == 0:
         for name, score in true_scores_this_step.items():
             average_true_scores[name] = score
    else:
        for name, score in true_scores_this_step.items():
            # Formula for running average: new_avg = old_avg + (new_value - old_avg) / count
            # Count for the new average will be current_step + 1
            old_avg = average_true_scores[name]
            average_true_scores[name] = old_avg + (score - old_avg) / (current_step + 1)

    simulation_step() # This updates global state like active_originator, credibilities, etc. and increments current_step

    # --- Animation Drawing ---
    # Small pause duration for animation phases
    phase_pause = 0.05

    # 0. Reset lines from previous frame
    originator_line.set_visible(False)
    for line in db_to_cv_lines: line.set_visible(False)
    for line in cv_to_db_lines: line.set_visible(False)

    # Update Visuals (Scores, Text, Validator Colors/Credibility, Turn Counts)
    title_text.set_text(f"Test {last_test_id}: V{active_originator} originates, others cross-validate")
    step_text.set_text(f"Step: {current_step}")

    # Update turn count display
    for i in range(N_VALIDATORS):
        radio_turn_texts[i].set_text(f"({validator_turn_counts[i]})")

    # Update validator colors and alpha (credibility) - Set base state first
    for i in range(N_VALIDATORS):
        cred = validators[i]['credibility']
        alpha = max(0.1, cred) # Use credibility for alpha, with a minimum visibility
        is_bad = validators[i]['is_bad_actor']
        ec = 'darkred' if is_bad else 'black'
        lw = 3 if is_bad else 1 # Line width for bad actor highlight

        # Set default color (gray) before highlighting active roles
        validator_circles[i].set_facecolor('gray')
        validator_circles[i].set_alpha(alpha)
        validator_circles[i].set_edgecolor(ec)
        validator_circles[i].set_linewidth(lw)
        validator_texts[i].set_text(f"V{i}\nC: {cred:.2f}")
        text_color = 'black' if alpha > 0.6 else 'white'
        validator_texts[i].set_color(text_color)

    # Update model scores display below the database and position arrows
    arrow_x_offset = -0.20 # Increased negative offset further
    target_arrow.set_visible(False) # Hide initially each frame
    victim_arrow.set_visible(False) # Hide initially each frame
    for i, name in enumerate(MODEL_NAMES):
        score_text_obj = model_score_texts[i]
        final_score = final_model_scores[name]
        avg_true = average_true_scores[name]
        error_percent = abs(final_score - avg_true) * 100
        score_text_obj.set_text(f"{name}: {final_score:.3f} (True: {avg_true:.3f} / Err: {error_percent:.1f}%)")
        x_pos, y_pos = score_text_obj.get_position()

        if name == TARGET_MODEL:
            target_arrow.set_position((x_pos + arrow_x_offset, y_pos))
            target_arrow.set_visible(True)
        elif name == VICTIM_MODEL:
            victim_arrow.set_position((x_pos + arrow_x_offset, y_pos))
            victim_arrow.set_visible(True)

    # --- Communication Animation Phases ---
    line_anim_steps = 5
    line_anim_pause = 0.01 # Pause between drawing segments of a line

    # Phase 1: Originator submits to DB
    orig_center = validator_pos[active_originator]
    start_point_orig = get_circle_edge_point(orig_center, validator_radius, db_center)
    end_point_db_from_orig = get_rect_edge_point(db_center, db_width, db_height, orig_center)
    validator_circles[active_originator].set_facecolor('red') # Highlight originator
    originator_line.set_linestyle('--')
    originator_line.set_color('red')
    # Use the single-line animation helper (or adapt multi-line for one line)
    animate_line_segments([originator_line], [start_point_orig], [end_point_db_from_orig], steps=line_anim_steps, pause_duration=line_anim_pause)


    # Phase 2: DB requests from Cross-Validators (Simultaneous)
    originator_line.set_visible(False) # Hide originator line after its animation
    db_start_points = []
    cv_end_points = []
    active_db_to_cv_lines = []
    for idx, cv_id in enumerate(cross_validator_ids_this_step):
        cv_center = validator_pos[cv_id]
        validator_circles[cv_id].set_facecolor('orange') # Highlight CVs being requested
        start_point_db = get_rect_edge_point(db_center, db_width, db_height, cv_center)
        end_point_cv = get_circle_edge_point(cv_center, validator_radius, db_center)
        db_start_points.append(start_point_db)
        cv_end_points.append(end_point_cv)
        line = db_to_cv_lines[idx]
        line.set_linestyle(':')
        line.set_color('blue')
        active_db_to_cv_lines.append(line)
    # Animate all DB->CV lines together
    animate_line_segments(active_db_to_cv_lines, db_start_points, cv_end_points, steps=line_anim_steps, pause_duration=line_anim_pause)


    # Phase 3: Cross-Validators submit to DB (Simultaneous)
    for line in db_to_cv_lines: line.set_visible(False) # Hide request lines
    cv_start_points = []
    db_end_points = []
    active_cv_to_db_lines = []
    for idx, cv_id in enumerate(cross_validator_ids_this_step):
        cv_center = validator_pos[cv_id]
        validator_circles[cv_id].set_facecolor('blue') # Highlight CVs submitting
        start_point_cv = get_circle_edge_point(cv_center, validator_radius, db_center)
        end_point_db = get_rect_edge_point(db_center, db_width, db_height, cv_center)
        cv_start_points.append(start_point_cv)
        db_end_points.append(end_point_db)
        line = cv_to_db_lines[idx]
        line.set_linestyle('-')
        line.set_color('blue')
        active_cv_to_db_lines.append(line)
    # Animate all CV->DB lines together
    animate_line_segments(active_cv_to_db_lines, cv_start_points, db_end_points, steps=line_anim_steps, pause_duration=line_anim_pause)

    # Main pause based on slider (consider the time taken by line animations)
    total_line_anim_time = (1 + len(cross_validator_ids_this_step) * 2) * line_anim_steps * line_anim_pause
    adjusted_pause = max(0.01, simulation_speed - total_line_anim_time)
    plt.pause(adjusted_pause)

    # Reset active roles visually (logic is handled by simulation_step)
    # Colors will be reset at the start of the next frame
    # Lines will be hidden at the start of the next frame

    # Return updated artists
    updated_artists = (validator_circles + validator_texts +
                       [db_rect, db_text] + model_score_texts +
                       [step_text, title_text, target_arrow, victim_arrow] +
                       radio_turn_texts + # Add turn count texts
                       [originator_line] + db_to_cv_lines + cv_to_db_lines)
    return updated_artists


# --- Run Simulation ---
# Use plt.show(block=False) and a loop for better control with plt.pause
plt.show(block=False)
plt.pause(0.1) # Initial short pause

# Keep running animation manually
frame_count = 500 # Run for a fixed number of steps for demo purposes
for frame in range(frame_count):
    if not plt.fignum_exists(fig.number): # Check if the figure window is closed
        print("Plot window closed, stopping simulation.")
        break
    animate(frame)

print("\nSimulation Finished.")
print("Final Credibilities:")
for i in range(N_VALIDATORS):
    print(f"  Validator {i}: {validators[i]['credibility']:.3f} {'(Bad Actor)' if validators[i]['is_bad_actor'] else ''}")

print("\nFinal Model Scores:")
for name, score in sorted(final_model_scores.items(), key=lambda item: item[1], reverse=True):
    print(f"  {name}: {score:.3f}")

# Keep the plot open until manually closed after the loop finishes
plt.show()
